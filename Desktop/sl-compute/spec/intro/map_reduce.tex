\section{Parallelism}

Multi-Party Computation protocols take place in a distributed network
in which there is latency in the communication channels between parties.
As such, the number of rounds of communication is an important metric
of efficiency, and can often become the bottle-neck.
In order to reduce the round complexity, it is often necessary to make
use of parallelism.
That is, if there are multiple sub-protocols which are independent of
each other, it is important that these can execute concurrently.

\subsection{Map}

In the Silent Compute framework, we use the \textit{map} higher-level function
which captures many of the cases in which parallelism is needed for MPC.
Given an ABB protocol that executes on some ABB-object of type A
and outputs another ABB-object of type B,
\textit{map} applies the function on an array of type A objects
to create an array of type B objects.

For instance, the function $A2B$ takes an Arithmetic sharing of a
non-negative integer and converts it to a Boolean sharing of the integer.
If we had an array of Arithmetic-shared integers $\avec{X}$,
we could convert it to an array of Boolean-shared integers as follows:
$\bvec{X} = map(A2B, \avec{X})$.

If $f$ is some function that takes $t>1$ parameters,
\textit{map} can be given the function $f$ followed by
$t$ arrays of inputs, each of length $n$.
\textit{map} then returns an array of length $n$,
in which the i$^{th}$ item is obtained by applying $f$
on the i$^{th}$ item of each array.
For instance, $\bitvec{Z} = map(\wedge, \bitvec{X}, \bitvec{Y})$
computes the bit-wise AND of two bit arrays. 
In the case where some parameters are arrays and some are singletons,
\textit{map} will treat the singletons as if they are an
array that replicates the singleton value.
For instance $\bitvec{Z} = map(\wedge, \bit{b}, \bitvec{X})$
will return the all-zero bit-vector if $b=0$,
and will return $\bitvec{X}$ otherwise.

\subsection{Reduce}
Another common task that can be parallelized is the $\emph{reduce}$
higher-order function, which applies an associative 
binary operation to combine all items of an array.
Consider for instance, the product of an array of integers,
or the OR of an array of bits.
Concretely, let $\vec{X} = [\vec{X}_1, \ldots, \vec{X}_n]$ be the array, 
and $\circ$ the binary operation.
The \emph{reduce} function computes $\vec{X}_1 \circ \ldots  \circ \vec{X}_n$.
To implement \emph{reduce} efficiently, 
we can take advantage of parallelization using the \textit{map} higher-order function.
We first define a mapping function which takes a pair of items 
$(\vec{X}_i, \vec{X}_{i+1})$ and outputs $\vec{X}_i \circ \vec{X}_{i+1}$.
We then combine adjacent elements into pairs and apply the \textit{map} function to each pair.
This results in an array $\vec{X}' = \vec{X}'_1, \ldots, \vec{X}'_{\lceil n/2 \rceil}$, 
for which $\vec{X}'_1 \circ \ldots \circ \vec{X}'_{\lceil n/2 \rceil} =
\vec{X_1} \circ \ldots  \circ \vec{X_n}$.
If this is repeated $\lceil \log(n) \rceil$ times, the output will be a single element
with value equal to $\vec{X_1} \circ \ldots  \circ \vec{X_n}$.
This ``tree-like'' application of \textit{map}, implements \textit{reduce} 
with a $\lceil log(n) \rceil$-factor increase in the round complexity
relative to the cost of a single call to the protocol implementing $\circ$.

